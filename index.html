<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Bootstrapping World Model</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&display=swap" rel="stylesheet">
  <style>
    body {
      margin: 0;
      font-family: 'Inter', sans-serif;
      color: #eaeaea;
      background-color: #121212;
      line-height: 1.6;
    }
    header {
      background: url('your-northern-lights-background.jpg') no-repeat center center/cover;
      height: 100vh;
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
      text-align: center;
      color: white;
      padding: 0 1em;
    }
    h1 {
      font-size: 4.5em;
      font-weight: 800;
      margin: 0;
    }
    h2 {
      font-size: 1.5em;
      font-weight: 400;
      margin-top: 1em;
    }
    .buttons {
      margin-top: 2em;
    }
    .buttons a {
      text-decoration: none;
      margin: 0 0.5em;
      padding: 0.75em 1.5em;
      border-radius: 8px;
      background-color: #1f1f1f;
      color: #eaeaea;
      border: 1px solid #444;
      font-weight: 600;
      transition: background 0.3s, transform 0.3s;
    }
    .buttons a:hover {
      background-color: #2e2e2e;
      transform: scale(1.05);
    }
    section {
      padding: 4em 1em;
      max-width: 900px;
      margin: auto;
    }
    section h3 {
      font-size: 2em;
      font-weight: 600;
      border-bottom: 1px solid #333;
      padding-bottom: 0.3em;
      margin-bottom: 1em;
    }
    section p {
      font-size: 1.1em;
      color: #cccccc;
    }
    section img, section video {
      max-width: 100%;
      border-radius: 12px;
      margin-top: 1.5em;
      box-shadow: 0 0 10px rgba(0,0,0,0.5);
    }
  </style>
</head>
<body>
  <header>
    <h1>Bootstrapping World Models</h1>
    <h2>from Dynamics Models in Multimodal Foundation Models</h2>
    <div class="buttons">
      <a href="LINK_TO_PAPER" target="_blank">Paper üìú</a>
      <a href="https://github.com/yfqiu-nlp/vlm-world-model" target="_blank">GitHub üíª</a>
      <a href="https://huggingface.co/yfqiu-nlp/chameleon-world-model-aurora" target="_blank">Model ü§ó</a>
    </div>
  </header>

  <section>
    <h3>üéûÔ∏è High-Level Overview</h3>
    <img src="./figures/vlm-world-model.gif" alt="VLM World Model Demo" style="max-width:100%;">
  </section>

  <section>
    <h3>üìÑ Abstract</h3>
    <p>
      To what extent do vision-and-language foundation models possess a realistic world model (observation √ó action ‚Üí observation) and a dynamics model (observation √ó observation ‚Üí action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a loss-weighting mechanism for the image tokens weighted by the its importance predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on AURORA-BENCH. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of 15% on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of AURORA-BENCH.
    </p>
  </section>

  <section>
    <h3>üåå Qualitative Examples in AURORA</h3>
    <img src="figures/qualitative-example.png" alt="Qualitative Examples from AURORA" />
  </section>

  <section>
    <h3>üèûÔ∏è Real-World Qualitative Examples</h3>
    <img src="figures/real-world-qualitative-example.png" alt="Real World Example 1" />
  </section>

  <section>
    <h3>‚ö†Ô∏è Limitations</h3>
    <p>
      While our approach demonstrates the effectiveness of our approaches across AURORA-BENCH, the authors would like to highlight few limitations we have discovered:
      ‚Ä¢ Despite efforts to guide the model via supervised fine-tuning with loss weighting or inference-time verification (Table 2), we observe that the model may still resort to copying the source observation, especially under low sampling temperatures or ambiguous instructions.
      ‚Ä¢ While we show preliminary results of language-steered observation prediction, fine-grained control remains limited, and understanding subtle instructions (e.g., spatial or quantitative edits) remains challenging.
      ‚Ä¢ We observe variance across different runs of experiments, likely due to the sensitivity of sampling for generation in multimodal models. To address this, we report results averaged over multiple runs and include performance under the best-of-N sampling distribution during inference for a robust comparison.
      ‚Ä¢ We mostly conduct experiments using the native and unified VLM, Chameleon, as it is currently the only open-source VLM that supports interleaved image-text generation by default. This choice allows for fair and consistent benchmarking across our tasks. Moreover, Chameleon has demonstrated competitive performance in our settings. For example, its results are comparable to VILA-U in our dynamics prediction task. Future work should explore the generalisation to other multimodal foundation models with stronger capabilities.
    </p>
  </section>
</body>
</html>
