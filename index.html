<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Bootstrapping World Model</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&display=swap" rel="stylesheet">
  <style>
    body {
      margin: 0;
      font-family: 'Inter', sans-serif;
      color: #1a1a1a;
      background-color: #ffffff;
      line-height: 1.6;
    }
    header {
      background-color: #f4f4f4;
      height: 60vh;
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
      text-align: center;
      padding: 0 1em;
      border-bottom: 1px solid #ddd;
    }
    h1 {
      font-size: 3.5em;
      font-weight: 800;
      margin: 0;
    }
    h2 {
      font-size: 1.5em;
      font-weight: 400;
      margin-top: 1em;
    }
    .buttons {
      margin-top: 2em;
      display: flex;
      gap: 1em;
    }
    .buttons a {
      text-decoration: none;
      padding: 0.75em 1.8em;
      border-radius: 12px;
      background: linear-gradient(135deg, #4f46e5, #3b82f6);
      color: #ffffff;
      font-weight: 600;
      box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
      transition: all 0.3s ease-in-out;
      font-size: 1em;
    }
    .buttons a:hover {
      background: linear-gradient(135deg, #4338ca, #2563eb);
      transform: translateY(-2px);
      box-shadow: 0 6px 16px rgba(0, 0, 0, 0.15);
    }
    section {
      padding: 4em 1em;
      max-width: 900px;
      margin: auto;
    }
    section h3 {
      font-size: 2em;
      font-weight: 600;
      border-bottom: 2px solid #eee;
      padding-bottom: 0.3em;
      margin-bottom: 1em;
    }
    section p {
      font-size: 1.1em;
      color: #333;
    }
    section img, section video {
      max-width: 100%;
      border-radius: 12px;
      margin-top: 1.5em;
      box-shadow: 0 2px 6px rgba(0, 0, 0, 0.1);
    }
  </style>
</head>

<body>
  <header>
    <h1>Bootstrapping World Models</h1>
    <h2>from Dynamics Models in Multimodal Foundation Models</h2>
    <div class="buttons">
      <a href="LINK_TO_PAPER" target="_blank">Paper üìú</a>
      <a href="https://github.com/yfqiu-nlp/vlm-world-model" target="_blank">GitHub üíª</a>
      <a href="https://huggingface.co/yfqiu-nlp/chameleon-world-model-aurora" target="_blank">Model ü§ó</a>
    </div>
  </header>

  <section>
    <h3>üéûÔ∏è High-Level Overview</h3>
    <img src="./figures/vlm-world-model.gif" alt="VLM World Model Demo" style="max-width:100%;">
  </section>

  <section>
    <h3>üìÑ Abstract</h3>
    <p>
      To what extent do vision-and-language foundation models possess a realistic world model (observation √ó action ‚Üí observation) and a dynamics model (observation √ó observation ‚Üí action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a loss-weighting mechanism for the image tokens weighted by the its importance predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on AURORA-BENCH. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of 15% on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of AURORA-BENCH.
    </p>
  </section>

  <section>
    <h3>üåå Qualitative Examples in AURORA</h3>
    <img src="figures/qualitative-example.png" alt="Qualitative Examples from AURORA" />
  </section>

  <section>
    <h3>üèûÔ∏è Real-World Qualitative Examples</h3>
    <img src="figures/real-world-qualitative-example.png" alt="Real World Example 1" />
  </section>

  <section>
  <h3>‚ö†Ô∏è Limitations</h3>
  <p>
    While our approach demonstrates effectiveness across AURORA-BENCH, the authors highlight key limitations:
  </p>
  <ul>
    <li>
      Despite efforts to guide the model via supervised fine-tuning with loss weighting or inference-time verification (Table 2), we observe that the model may still resort to copying the source observation, especially under low sampling temperatures or ambiguous instructions.
    </li>
    <li>
      While we show preliminary results of language-steered observation prediction, fine-grained control remains limited, and understanding subtle instructions (e.g., spatial or quantitative edits) remains challenging.
    </li>
    <li>
      We observe variance across different runs of experiments, likely due to the sensitivity of sampling for generation in multimodal models. To address this, we report results averaged over multiple runs and include performance under the best-of-N sampling distribution during inference for robust comparison.
    </li>
    <li>
      We primarily experiment with the native VLM Chameleon, as it is currently the only open-source VLM supporting interleaved image-text generation by default. This enables consistent benchmarking, and Chameleon shows competitive performance (e.g., comparable to VILA-U in dynamics prediction). Future work should explore generalization to stronger multimodal foundation models.
    </li>
  </ul>
</section>

  <footer>
    <p style="text-align: center; color: #888;">¬© 2025 VLM World Models</p>
  </footer>
</body>
</html>
