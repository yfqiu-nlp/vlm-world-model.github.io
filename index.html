<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Bootstrapping World Model</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&display=swap" rel="stylesheet">
  <style>
    body {
      margin: 0;
      font-family: 'Inter', sans-serif;
      color: #eaeaea;
      background-color: #121212;
      line-height: 1.6;
    }
    header {
      background: url('your-northern-lights-background.jpg') no-repeat center center/cover;
      height: 100vh;
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
      text-align: center;
      color: white;
      padding: 0 1em;
    }
    h1 {
      font-size: 4.5em;
      font-weight: 800;
      margin: 0;
    }
    h2 {
      font-size: 1.5em;
      font-weight: 400;
      margin-top: 1em;
    }
    .buttons {
      margin-top: 2em;
    }
    .buttons a {
      text-decoration: none;
      margin: 0 0.5em;
      padding: 0.75em 1.5em;
      border-radius: 8px;
      background-color: #1f1f1f;
      color: #eaeaea;
      border: 1px solid #444;
      font-weight: 600;
      transition: background 0.3s, transform 0.3s;
    }
    .buttons a:hover {
      background-color: #2e2e2e;
      transform: scale(1.05);
    }
    section {
      padding: 4em 1em;
      max-width: 900px;
      margin: auto;
    }
    section h3 {
      font-size: 2em;
      font-weight: 600;
      border-bottom: 1px solid #333;
      padding-bottom: 0.3em;
      margin-bottom: 1em;
    }
    section p {
      font-size: 1.1em;
      color: #cccccc;
    }
    section img, section video {
      max-width: 100%;
      border-radius: 12px;
      margin-top: 1.5em;
      box-shadow: 0 0 10px rgba(0,0,0,0.5);
    }
  </style>
</head>
<body>
  <header>
    <h1>Bootstrapping World Models</h1>
    <h2>from Dynamics Models in Multimodal Foundation Models</h2>
    <div class="buttons">
      <a href="LINK_TO_PAPER" target="_blank">Paper üìú</a>
      <a href="https://github.com/yfqiu-nlp/vlm-world-model" target="_blank">GitHub üíª</a>
      <a href="https://huggingface.co/yfqiu-nlp/chameleon-world-model-aurora" target="_blank">Model ü§ó</a>
    </div>
  </header>

  <section>
    <h3>üéûÔ∏è High-Level Overview</h3>
    <video src="figures/vlm-world-model.gif" autoplay loop muted></video>
  </section>

  <section>
    <h3>üìÑ Abstract</h3>
    <p>
      To what extent do vision-and-language foundation models possess a realistic world model (observation √ó action ‚Üí observation) and a dynamics model (observation √ó observation ‚Üí action), when actions are expressed through language? While open-source foundation models struggle with both, we find that fine-tuning them to acquire a dynamics model through supervision is significantly easier than acquiring a world model. In turn, dynamics models can be used to bootstrap world models through two main strategies: 1) weakly supervised learning from synthetic data and 2) inference time verification. Firstly, the dynamics model can annotate actions for unlabelled pairs of video frame observations to expand the training data. We further propose a loss-weighting mechanism for the image tokens weighted by the its importance predicted by a recognition model. Secondly, the dynamics models can assign rewards to multiple samples of the world model to score them, effectively guiding search at inference time. We evaluate the world models resulting from both strategies through the task of action-centric image editing on AURORA-BENCH. Our best model achieves a performance competitive with state-of-the-art image editing models, improving on them by a margin of 15% on real-world subsets according to GPT4o-as-judge, and achieving the best average human evaluation across all subsets of AURORA-BENCH.
    </p>
  </section>

  <section>
    <h3>üåå Qualitative Examples in AURORA</h3>
    <img src="figures/qualitative-example.png" alt="Qualitative Examples from AURORA" />
  </section>

  <section>
    <h3>üèûÔ∏è Real-World Qualitative Examples</h3>
    <img src="figures/real-world-qualitative-example.png" alt="Real World Example 1" />
  </section>

  <section>
    <h3>‚ö†Ô∏è Limitations</h3>
    <p>
      Despite strong performance, our model exhibits limitations in highly occluded scenes, long-horizon temporal reasoning, and robustness to distribution shifts. These areas represent key directions for future work.
    </p>
  </section>
</body>
</html>
